{"cells":[{"cell_type":"code","source":["# Install required packages\n","%pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community langchain-openai azure-identity python-dotenv pydub azure-cognitiveservices-speech --quiet"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fedc7b9d-6b97-4418-b225-6cccd9a3a873"},{"cell_type":"code","source":["import os\n","import certifi\n","import base64\n","import tempfile\n","import time\n","import re \n","import datetime\n","import json\n","from pathlib import Path\n","\n","from crewai import Agent, Task, Crew\n","from crewai_tools import tool\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig, ResultReason, CancellationReason, SpeechSynthesisOutputFormat\n","from langchain.schema import HumanMessage, AIMessage\n","from langchain_openai import AzureChatOpenAI\n","from dotenv import load_dotenv\n","\n","import sempy.fabric as fabric # Added for DAX query"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c744b89-dbae-4a68-b3d2-d5f878c2d7a4"},{"cell_type":"code","source":["\n","# ===================== CONFIGURABLE VARIABLES =====================\n","# --- Input Configuration ---\n","# Load environment variables\n","load_dotenv(dotenv_path=\"/lakehouse/default/Files/.env\") #Example: \"/lakehouse/default/Files/.env\"\n","# =================================================================="],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c8d6cd3-d3a1-4ea4-94c4-ab36649b7f52"},{"cell_type":"code","source":["# Set the environment variable 'REQUESTS_CA_BUNDLE' to the path of the certificate bundle provided by certifi\n","os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n","# Set the environment variable 'SSL_CERT_FILE' to the path of the certificate bundle provided by certifi\n","os.environ['SSL_CERT_FILE'] = certifi.where()\n","\n","# Print environment variables (remove in production)\n","print(\"Checking environment variables:\")\n","print(f\"Endpoint exists: {'MY_AZURE_OPENAI_ENDPOINT' in os.environ}\")\n","print(f\"API Key exists: {'AZURE_OPENAI_KEY' in os.environ}\")\n","print(f\"Deployment exists: {'AZURE_OPENAI_MODEL_DEPLOYMENT' in os.environ}\")\n","\n","# Get configuration\n","api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n","azure_endpoint = os.getenv(\"MY_AZURE_OPENAI_ENDPOINT\")\n","deployment_name = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n","api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n","\n","# Print configurations (remove sensitive data in production)\n","print(\"\\nConfiguration values:\")\n","print(f\"Endpoint: {azure_endpoint}\")\n","print(f\"Deployment: {deployment_name}\")\n","print(f\"API Key length: {len(api_key) if api_key else 0}\")\n","print(f\"Api version: {api_version}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20fdb705-7f2a-46b7-8225-950631327772"},{"cell_type":"code","source":["# Initialize Azure OpenAI model\n","llm = AzureChatOpenAI(\n","    azure_endpoint=azure_endpoint,\n","    api_key=api_key,\n","    azure_deployment=deployment_name,\n","    api_version=api_version,\n",")\n","\n","# Test the connection\n","try:\n","    print(\"\\nTesting Azure OpenAI connection...\")\n","    response = llm.invoke(\"Hello! This is a test message.\")\n","    print(\"Connection successful!\")\n","    print(f\"Response: {response}\")\n","except Exception as e:\n","    print(f\"Error: {str(e)}\")\n","    print(f\"Error type: {type(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81556669-75f2-4a7d-9349-144fdf93460e"},{"cell_type":"code","source":["# Assuming 'llm' is already initialized in your environment\n","global_llm = llm # Make sure llm is assigned here"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"54bda926-120e-4acd-b8ed-18b9820e7b14"},{"cell_type":"code","source":["# Azure Speech Service Configuration\n","# IMPORTANT: Ensure the environment variables AZURE_SPEECH_KEY and AZURE_SPEECH_REGION are set in your environment.\n","AZURE_SPEECH_ENDPOINT= os.getenv(\"AZURE_SPEECH_ENDPOINT\")\n","AZURE_SPEECH_KEY =  os.getenv(\"AZURE_SPEECH_KEY\") # e.g., set AZURE_SPEECH_KEY=\"your_azure_speech_key\" in the .env file\n","AZURE_SPEECH_REGION =  os.getenv(\"AZURE_SPEECH_REGION\") # e.g., set AZURE_SPEECH_REGION=\"yourazureregion\" in the .env file\n","AZURE_SPEECH_LANG = \"en-US\" # Default fallback\n","\n","# Voices for the podcast hosts\n","# Find available voice names here: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts\n","AZURE_SPEECH_VOICE_1 = \"en-US-Emma2:DragonHDLatestNeural\" # Example: \"en-US-Emma2:DragonHDLatestNeural\" or \"en-AU-WilliamNeural\"\n","AZURE_SPEECH_VOICE_2 = \"en-US-Andrew3:DragonHDLatestNeural\"   # Example: \"en-US-Andrew3:DragonHDLatestNeural\" or \"en-AU-NatashaNeural\"\n","\n","# Derive language automatically from the first voice for SSML tag\n","try:\n","    # Check if AZURE_SPEECH_VOICE_1 is not None or empty before attempting split\n","    if AZURE_SPEECH_VOICE_1 and isinstance(AZURE_SPEECH_VOICE_1, str) and AZURE_SPEECH_VOICE_1.startswith('<'):\n","        print(f\"Warning: AZURE_SPEECH_VOICE_1 seems to contain a placeholder '{AZURE_SPEECH_VOICE_1}'. Using default language '{AZURE_SPEECH_LANG}'. Please replace the placeholder.\")\n","    elif AZURE_SPEECH_VOICE_1 and isinstance(AZURE_SPEECH_VOICE_1, str):\n","         parts = AZURE_SPEECH_VOICE_1.split('-')\n","         if len(parts) >= 2:\n","             AZURE_SPEECH_LANG = f\"{parts[0]}-{parts[1]}\"\n","         else:\n","              print(f\"Warning: Could not reliably determine language from voice '{AZURE_SPEECH_VOICE_1}'. Using default '{AZURE_SPEECH_LANG}'.\")\n","    elif AZURE_SPEECH_VOICE_1 is None:\n","         print(f\"Warning: AZURE_SPEECH_VOICE_1 is not set. Using default language '{AZURE_SPEECH_LANG}'.\")\n","\n","except Exception as e:\n","    print(f\"Warning: Error parsing voice name for language '{AZURE_SPEECH_VOICE_1}': {e}. Using default '{AZURE_SPEECH_LANG}'.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"730e0053-4bca-4363-bd24-f7cf2012490c"},{"cell_type":"code","source":["\n","import datetime\n","# Directory path where the final audio file will be saved. Ensure this path exists or is writable. Must end with a forward slash '/'.\n","DEFAULT_OUTPUT_FILEPATH = \"/lakehouse/default/Files/podcastreport/\" # Example: \"/lakehouse/default/Files/podcastreport/\"\n","OUTPUT_FILENAME = \"dax_sales_summary\" # todo - change to use the name of input image file\n","timestamp = int(datetime.datetime.now().timestamp())\n","OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}_{timestamp}\"\n","# Construct the full output file path\n","DEFAULT_OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}.wav\"\n","OUTPUT_FILE_PATH = DEFAULT_OUTPUT_FILEPATH + DEFAULT_OUTPUT_FILENAME\n","\n","# MIME type mappings (Generally constant)\n","MIME_TYPES = {\n","    \".png\": \"image/png\",\n","    \".jpg\": \"image/jpeg\",\n","    \".jpeg\": \"image/jpeg\",\n","    \".gif\": \"image/gif\",\n","    \".webp\": \"image/webp\"\n","}\n","# =================================================================="],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24ba76bd-58c7-4c83-aa0b-c119deb84422"},{"cell_type":"code","source":["\n","print(f\"Azure Speech Region: {AZURE_SPEECH_REGION}\")\n","if not AZURE_SPEECH_KEY:\n","    print(\"Warning: AZURE_SPEECH_KEY environment variable not found or empty.\")\n","    # Optionally raise an error\n","    # raise ValueError(\"Azure Speech Key (AZURE_SPEECH_KEY) environment variable not set.\")\n","if not AZURE_SPEECH_REGION:\n","    print(\"Warning: AZURE_SPEECH_REGION environment variable not found or empty.\")\n","    # Optionally raise an error\n","    # raise ValueError(\"Azure Speech Region (AZURE_SPEECH_REGION) environment variable not set.\")\n","\n","print(f\"Host 1 Voice: {AZURE_SPEECH_VOICE_1}\")\n","print(f\"Host 2 Voice: {AZURE_SPEECH_VOICE_2}\")\n","print(f\"Derived SSML Language: {AZURE_SPEECH_LANG}\")\n","print(f\"Output Base Filename: {OUTPUT_FILENAME}\")\n","print(f\"Output Directory: {DEFAULT_OUTPUT_FILEPATH}\")\n","print(f\"Full Output Path: {OUTPUT_FILE_PATH}\")\n","print(f\"----------------------------\")\n","\n","# Pre-flight check for output directory\n","output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n","if output_dir and not os.path.exists(output_dir):\n","    print(f\"Warning: Output directory '{output_dir}' does not exist. Attempting to create it.\")\n","    try:\n","        os.makedirs(output_dir, exist_ok=True) # Use exist_ok=True\n","        print(f\"Successfully created output directory: {output_dir}\")\n","    except OSError as e:\n","        print(f\"Error: Failed to create output directory '{output_dir}': {e}\")\n","        raise # Re-raise the error to stop execution if directory creation fails\n","# --- End Checks ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"023dff5a-abc3-4966-b8aa-8460ab8d1bf8"},{"cell_type":"code","source":["@tool(\"SSML Formatter Tool\")\n","def format_ssml_tool(dialogue_content: str) -> str:\n","    \"\"\"\n","    Takes podcast dialogue text (expected to contain <voice> tags already)\n","    and wraps it in the necessary <speak> root element for Azure SSML,\n","    dynamically setting the xml:lang attribute based on the configured primary voice.\n","    It cleans up potential extra whitespace or markdown code fences.\n","    Provide the raw dialogue content generated by the scriptwriter.\n","    \"\"\"\n","    # --- (format_ssml_tool) ---\n","    print(\"SSML Formatter Tool: Received dialogue content.\")\n","    ssml_body = str(dialogue_content).strip()\n","\n","    # Remove potential markdown code fences added by LLM\n","    ssml_body = re.sub(r'^```xml\\s*', '', ssml_body, flags=re.IGNORECASE | re.MULTILINE)\n","    ssml_body = re.sub(r'\\s*```$', '', ssml_body, flags=re.MULTILINE)\n","    ssml_body = ssml_body.strip()\n","\n","    lang_attribute = f'xml:lang=\"{AZURE_SPEECH_LANG}\"'\n","    is_already_wrapped = ssml_body.startswith('<speak') and ssml_body.endswith('</speak>')\n","    has_correct_lang = lang_attribute in ssml_body[:150]\n","\n","    if is_already_wrapped and has_correct_lang:\n","        print(f\"SSML Formatter Tool: Input already correctly wrapped with {lang_attribute}.\")\n","        return ssml_body\n","    elif is_already_wrapped and not has_correct_lang:\n","        print(f\"SSML Formatter Tool: Input wrapped, but ensuring correct {lang_attribute}.\")\n","        pattern = re.compile(r'(<speak[^>]*)(\\s*xml:lang=\"[^\"]*\")?([^>]*>)', re.IGNORECASE)\n","        if pattern.search(ssml_body):\n","             ssml_body = pattern.sub(rf'\\1 {lang_attribute}\\3', ssml_body, count=1)\n","        else:\n","             ssml_body = ssml_body.replace('<speak', f'<speak {lang_attribute}', 1)\n","        return ssml_body\n","    else:\n","        print(f\"SSML Formatter Tool: Wrapping dialogue content with {lang_attribute}.\")\n","        ssml_full = f'<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" {lang_attribute}>\\n{ssml_body}\\n</speak>'\n","        return ssml_full\n","    # --- (End of format_ssml_tool code) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71d7106b-857d-4a14-a2e4-904c6b01022d"},{"cell_type":"code","source":["@tool(\"SSML to Speech Tool\")\n","def ssml_to_speech(ssml_input: str) -> str:\n","    \"\"\"\n","    Converts a **complete and valid** SSML string (including the root <speak> tag\n","    with correct xml:lang, and <voice> tags defining multiple voices) into speech\n","    and saves it as a single WAV file. Provide the full, validated SSML string.\n","    Use the 'SSML Formatter Tool' first to ensure validity.\n","    \"\"\"\n","    # --- (Keep the ssml_to_speech tool code as corrected previously) ---\n","    # ... (code from previous version for ssml_to_speech) ...\n","    try:\n","        # Credentials check\n","        speech_config = SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)\n","        speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)\n","\n","        ssml_content = str(ssml_input)\n","\n","        if not ssml_content.strip().startswith('<speak'):\n","            print(f\"Warning in ssml_to_speech: Input SSML might be invalid (missing <speak> tag): {ssml_content[:100]}...\")\n","\n","        print(f\"SSML to Speech Tool: Processing SSML (language should be embedded within): '{ssml_content[:150]}...'\")\n","        print(f\"Output will be saved to: {OUTPUT_FILE_PATH}\")\n","\n","        if not ssml_content:\n","            return \"Error: No SSML content provided for speech synthesis.\"\n","\n","        output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir); print(f\"Created output directory: {output_dir}\")\n","\n","        audio_config = AudioConfig(filename=OUTPUT_FILE_PATH)\n","        synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n","\n","        print(\"Synthesizing SSML...\")\n","        result = synthesizer.speak_ssml_async(ssml_content).get()\n","\n","        del synthesizer; del audio_config; time.sleep(0.1)\n","\n","        if result.reason == ResultReason.SynthesizingAudioCompleted:\n","            wait_time = 0\n","            while not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0) and wait_time < 5:\n","                time.sleep(0.5); wait_time += 0.5\n","            if not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0):\n","                 return f\"Error: Output file {OUTPUT_FILE_PATH} is missing or empty after synthesis completed.\"\n","            else:\n","                 print(f\"SSML synthesis successful. File saved to {OUTPUT_FILE_PATH} ({os.path.getsize(OUTPUT_FILE_PATH)} bytes).\")\n","                 return f\"Speech synthesis successful. File saved at: {OUTPUT_FILE_PATH}\"\n","        elif result.reason == ResultReason.Canceled:\n","            cancellation_details = result.cancellation_details\n","            error_message = f\"SSML synthesis canceled. Reason: {cancellation_details.reason}\"\n","            if cancellation_details.reason == CancellationReason.Error:\n","                print(f\"Cancellation Error Code: {cancellation_details.error_code}\")\n","                print(f\"Cancellation Error Details: {cancellation_details.error_details}\")\n","                error_message += f\" Error details: {cancellation_details.error_details}\"\n","            print(error_message)\n","            if os.path.exists(OUTPUT_FILE_PATH):\n","                try: os.remove(OUTPUT_FILE_PATH)\n","                except OSError as remove_err: print(f\"Warning: Could not remove partially created file {OUTPUT_FILE_PATH}: {remove_err}\")\n","            return error_message\n","        else:\n","             error_message = f\"Unexpected SSML synthesis result: {result.reason}\"\n","             print(error_message)\n","             if os.path.exists(OUTPUT_FILE_PATH):\n","                 try: os.remove(OUTPUT_FILE_PATH)\n","                 except OSError as remove_err: print(f\"Warning: Could not remove potentially invalid file {OUTPUT_FILE_PATH}: {remove_err}\")\n","             return error_message\n","\n","    except Exception as e:\n","        import traceback\n","        print(f\"Error in ssml_to_speech function: {str(e)}\")\n","        print(traceback.format_exc())\n","        if os.path.exists(OUTPUT_FILE_PATH):\n","            try: os.remove(OUTPUT_FILE_PATH)\n","            except OSError as remove_err: print(f\"Warning: Could not remove file {OUTPUT_FILE_PATH} during exception handling: {remove_err}\")\n","        return f\"Error during SSML to speech processing: {str(e)}\"\n","    # --- (End of ssml_to_speech code) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be164406-4ed4-42d8-9fdb-c87648b5f17e"},{"cell_type":"markdown","source":["# This section is added to define and use DAX queries as input for podcast generation\n","Prerequisites - Publish the semanticmodel used by DAX queries to the lakehouse which is asssigned to **fabricdataset** "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05aa0d62-3a8d-4882-ad82-10f51a2f044a"},{"cell_type":"code","source":["fabricdataset = \"wwilakehouse\"\n","# Define DAX queries as an array. This sample has 2 DAX queries added\n","queries = [\n","    \"\"\" DEFINE\n","\t                VAR __DS0Core = \n","\t\t            SUMMARIZECOLUMNS(\n","\t\t\t            'dimension_customer'[BuyingGroup],\n","\t\t\t            \"SumCustomerKey\", CALCULATE(SUM('dimension_customer'[CustomerKey]))\n","\t\t            )\n","\n","\t                VAR __DS0PrimaryWindowed = \n","\t\t            TOPN(1001, __DS0Core, [SumCustomerKey], 0, 'dimension_customer'[BuyingGroup], 1)\n","\n","                    EVALUATE\n","\t                __DS0PrimaryWindowed\n","\n","                    ORDER BY\n","\t                [SumCustomerKey] DESC, 'dimension_customer'[BuyingGroup]\n","                \"\"\",\n","    \"\"\"DEFINE\n","\tVAR __DS0Core = \n","\t\tSUMMARIZECOLUMNS(\n","\t\t\t'dimension_customer'[BuyingGroup],\n","\t\t\t\"SumLineageKey\", CALCULATE(SUM('dimension_customer'[LineageKey]))\n","\t\t)\n","\n","\tVAR __DS0PrimaryWindowed = \n","\t\tTOPN(1001, __DS0Core, [SumLineageKey], 0, 'dimension_customer'[BuyingGroup], 1)\n","\n","    EVALUATE\n","        __DS0PrimaryWindowed\n","\n","    ORDER BY\n","        [SumLineageKey] DESC, 'dimension_customer'[BuyingGroup]\n","\"\"\"\n","\n","]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"692eb0e0-b867-4e11-a48c-e8a99a330b27"},{"cell_type":"code","source":["def execute_dax_query(query):\n","    \"\"\"Executes a DAX query against the specified dataset.\n","\n","    Args:\n","        query: The DAX query string.\n","\n","    Returns:\n","        The query result as a recordset, or None on error.\n","    \"\"\"\n","    try:\n","        result = fabric.evaluate_dax(fabricdataset,query)\n","        print(f\"RESULT ==  {result}\")\n","        return result\n","    except Exception as e:\n","        print(f\"Error executing DAX query: {e}\")\n","        return None"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05aa12e1-e9e9-4b6e-bce9-dc637bd31321"},{"cell_type":"code","source":["def recordset_to_list(recordset):\n","    # if not recordset or not hasattr(recordset, 'rows') or not recordset.rows:\n","    #     return []\n","\n","    # if not hasattr(recordset, 'schema') or not recordset.schema:\n","    #     return []\n","    if recordset is None:\n","        return []\n","    try:\n","        # Try to use the to_dict method if it exists (for FabricDataFrame)\n","        if hasattr(recordset, 'to_dict') and callable(recordset.to_dict):\n","            data = recordset.to_dict(orient=\"records\")  # Get data as list of dicts\n","            if not data:\n","                return []\n","            else:\n","                return data\n","        elif hasattr(recordset, 'rows') and hasattr(recordset, 'schema'):\n","            # Fallback to the original method if 'to_dict' is not available\n","            columns = [field.name for field in recordset.schema]\n","            return [dict(zip(columns, row)) for row in recordset.rows]\n","        elif hasattr(recordset, 'rows'): # added this condition\n","             columns = [f\"column_{i+1}\" for i in range(len(recordset.rows[0]))]\n","             return [dict(zip(columns, row)) for row in recordset.rows]\n","        else:\n","            return []\n","    except Exception as e:\n","        print(f\"Error converting recordset to list: {e}\")\n","        return []"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f0c2b0f9-f5b9-4d62-8f97-1c40ccd19460"},{"cell_type":"code","source":["def combine_recordsets_to_json(dax_queries):\n","    \"\"\"Executes multiple DAX queries and combines the results into a JSON string.\n","\n","    Args:\n","        dataset_name: The name of the dataset to query.\n","        dax_queries: A list of DAX query strings.\n","\n","    Returns:\n","        A JSON string containing the combined results, or an empty JSON\n","        object if no recordsets had data.\n","    \"\"\"\n","    results = {}\n","    for i, query in enumerate(dax_queries):\n","        recordset = execute_dax_query(query)\n","        if recordset.empty:\n","            print(\"Empty recordset\")\n","        else:\n","            data_list = recordset_to_list(recordset)\n","            if data_list:  # Only add to results if the recordset has data\n","                results[f\"query_{i + 1}\"] = data_list\n","    return json.dumps(results) #Added indent for better json output"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"955ee591-1d17-480f-841c-395da1466df5"},{"cell_type":"code","source":["@tool(\"DAX executor and summarizer\")\n","def daxAgentFunc() :\n","    \"\"\"\n","    Execute the DAX queries and send the response to OpenAI to describe and summarize\n","    \"\"\"\n","    try:\n","        llm = global_llm\n","        if llm is None:\n","             return \"Error: LLM not provided to the tool. Make sure to set global_llm before using this tool.\"\n","        json_output = combine_recordsets_to_json(queries)\n","        #print(json_output)\n","        try:\n","            # Create a message with the recofrdsets\n","            message = HumanMessage(\n","                content=[\n","                    {\n","                        \"type\": \"text\",\n","                        \"text\": \"Please describe data in the JSON string, focusing on dimensions and respective values. Keep the scope of description to the data points available in the text\" \n","                    },\n","                    {\n","                        \"type\": \"text\",\n","                        \"text\": json_output\n","                    }\n","                ]\n","            )\n","            # Get the response from Azure OpenAI\n","            print(\"--- LLM Request ---- \")\n","            print (message)\n","            print(\"----------------------\")\n","            response = llm.invoke([message])\n","            print(\"Received response from Azure OpenAI\")\n","            # Extract the content from the response\n","            if isinstance(response, AIMessage):\n","                print(\"--- LLM Response ---- \")\n","                print (response.content)\n","                print(\"----------------------\")\n","                return response.content\n","            else:\n","                return str(response)\n","        except Exception as llm_error:\n","            print(f\"An error occurred in daxAgentFunc during LLM Call: {llm_error}\")\n","            return f\"Error in daxAgentFunc during LLM Call: {llm_error}\"      \n","    except Exception as e:\n","        print(f\"Error in daxAgentFunc before calling LLM: {e}\")\n","\n","# --- (End of executeDAXandSummarize) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"853e9ef9-8321-410d-9bc5-7f944cc33f55"},{"cell_type":"code","source":["# Placeholder to test the DAX queries output\n","#daxAgentFunc()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c9f3e73a-b6ed-45a9-856f-707d99fd9fab"},{"cell_type":"code","source":["# ======================= AGENTS =======================\n","\n","daxAgent = Agent(\n","    role=\"Semantic Model Analyst\",\n","    goal=\"Provide comprehensive and objective textual descriptions of recordsets, capturing all relevant details, especially focusing on text and numbers in the records.\",\n","    backstory=\"You're an expert at analyzing and describing data from recordsets with meticulous attention to detail, skilled at extracting factual information.\",\n","    verbose=True, allow_delegation=False, llm=global_llm, tools=[daxAgentFunc]\n",")    \n","\n","# --- NEW Report Analyzer Agent ---\n","report_analyzer = Agent(\n","    role=\"Business Report Analyzer\",\n","    goal=(\n","        \"Analyze the provided detailed image description (containing text, charts, data etc.) \"\n","        \"to extract key business insights, trends, figures, and potential implications. \"\n","        \"Synthesize these findings into a structured written analysis presented as an essay, \"\n","        \"starting with a concise Executive Summary.\"\n","        ),\n","    backstory=(\n","        \"You are a meticulous business analyst skilled at interpreting data presented visually (via its textual description). \"\n","        \"You excel at identifying significant patterns, summarizing complex information clearly, \"\n","        \"and presenting actionable findings in a well-structured report format suitable for executive review and communication planning.\"\n","        ),\n","    verbose=True,\n","    allow_delegation=False,\n","    llm=global_llm\n","    # This agent primarily works on text context, no specific tool needed beyond LLM capabilities.\n",")\n","\n","# --- Podcast Writer Agent (Now takes analysis as input) ---\n","podcast_writer = Agent(\n","    role=\"Podcast Dialogue Creator\",\n","    goal=(\n","        \"My main goal is to take that detailed business analysis report and turn it into a friendly, engaging chat between two podcast hosts. \"\n","        \"I'll write the script so it sounds natural, making sure to tag each host's lines correctly (using `<voice name='...'>` tags with the specific voices: \"\n","        f\"'{AZURE_SPEECH_VOICE_1}' for Host 1 and '{AZURE_SPEECH_VOICE_2}' for Host 2) so the final audio sounds great. \"\n","        \"I'll also do my best to wrap the whole thing in the main `<speak>` tags needed for the audio generation step.\"\n","    ),\n","    backstory=(\n","        \"Think of me as your creative partner who's great at taking serious reports and making them easy to understand and interesting to listen to. \"\n","        \"I specialize in writing natural-sounding conversations for two people, making sure all the important points from the analysis are covered clearly. \"\n","        \"I know how to set up the script with the right formatting (like those `<voice>` tags) so it's ready for the next step of actually creating the audio.\"\n","    ),\n","    verbose=True, allow_delegation=False, llm=global_llm\n",")\n","\n","# --- Speech Synthesizer Agent (Keep as before) ---\n","speech_synthesizer = Agent(\n","    role=\"SSML Formatting and Speech Synthesis Orchestrator\",\n","    goal=(\n","        \"Take raw podcast dialogue, format it into valid SSML using the SSML Formatter Tool (which sets the correct language based on configuration), \"\n","        \"and then convert the finalized SSML into a single high-quality spoken audio file using the SSML to Speech Tool.\"\n","        ),\n","    backstory=(\n","        \"You are an expert workflow manager for audio production. You first ensure the script is perfectly formatted as SSML with the correct language, \"\n","        \"then you use Azure's Text-to-Speech service to generate seamless, multi-voice audio output from the validated SSML.\"\n","        ),\n","    verbose=True, llm=global_llm, tools=[format_ssml_tool, ssml_to_speech]\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78a3b89e-8b67-4ced-98c4-d3e72753581b"},{"cell_type":"code","source":["# ======================= TASKS =======================\n","\n","dax_task = Task(\n","    description=(\n","        f\"Analyze the recordsets provided in the content. Provide a comprehensive and objective textual description \"\n","        \"detailing all dimensions and data. Do not include any other information other than what is available in the recordsets\"\n","        \"This description will be used by the Business Report Analyzer.\"\n","    ),\n","    agent=daxAgent,\n","    expected_output=\"A detailed, objective textual description of the recordsets, focusing on data and textual elements.\"\n",")\n","\n","# --- Analysis Task ---\n","analysis_task = Task(\n","    description=(\n","        \"Review the detailed  description provided in the context. \"\n","        \"Based *solely* on the information presented in that description (text, data points,  descriptions etc.), \"\n","        \"perform a thorough business analysis. Identify key insights, significant trends, important figures, and potential implications. \"\n","        \"Structure your findings as a written essay. Start with a clear 'Executive Summary' section summarizing the main points, \"\n","        \"followed by a more detailed 'Analysis' section elaborating on the findings. \"\n","        \"This report will be used to create a podcast script.\"\n","    ),\n","    agent=report_analyzer,\n","    expected_output=(\n","        \"A well-structured written analysis in essay format. \"\n","        \"It MUST begin with an 'Executive Summary' section. \"\n","        \"It MUST be followed by a detailed 'Analysis' section. \"\n","        \"The analysis must be based ONLY on the information from the input image description.\"\n","    ),\n","    context=[dax_task] # Depends on the image description\n",")\n","\n","# --- Business Update Task (it depends on analysis_task) ---\n","business_update_task = Task(\n","    description=(\n","        \"Okay, I have the business analysis report here (check the context). My task is to translate the key findings and insights from this report \"\n","        \"into a natural, back-and-forth dialogue for a podcast featuring Host 1 and Host 2. The conversation should flow well and make the analysis easy for listeners to grasp. \"\n","        \"\\n\\n**Formatting Guide for Audio:**\\n\" # Use markdown for emphasis\n","        f\"*   **Host 1:** Use the `<voice name=\\\"{AZURE_SPEECH_VOICE_1}\\\">...</voice>` tag for everything Host 1 says.\\n\"\n","        f\"*   **Host 2:** Use the `<voice name=\\\"{AZURE_SPEECH_VOICE_2}\\\">...</voice>` tag for everything Host 2 says.\\n\"\n","        \"*   **Structure:** Please alternate between these tags as the hosts speak. Getting these voice tags right is crucial for the audio step!\\n\"\n","        \"*   **Wrapping (Optional but helpful):** If you can, please also wrap the entire dialogue within the main `<speak>...</speak>` tags.\\n\"\n","        \"*   **Content:** Just include the words the hosts will actually say. No extra labels like 'Host 1:', notes about music, or markdown formatting around the SSML itself.\"\n","    ),\n","    agent=podcast_writer,\n","    expected_output=(\n","         \"A complete podcast script formatted as dialogue. It should feature alternating \"\n","         f\"`<voice name='{AZURE_SPEECH_VOICE_1}'>...</voice>` and `<voice name='{AZURE_SPEECH_VOICE_2}'>...</voice>` tags \"\n","         \"containing the spoken lines for each host, accurately reflecting the input analysis in a conversational style. \"\n","         \"Ideally, the whole script will be enclosed in `<speak>...</speak>` tags.\"\n","    ),\n","    context=[analysis_task] # Depends on the analysis report\n",")\n","\n","# --- Speech Task (it depends on business_update_task) ---\n","speech_task = Task(\n","    description=(\n","        \"Process the podcast dialogue script received from the previous task (available in context). \"\n","        \"Step 1: Use the 'SSML Formatter Tool' to ensure the script is wrapped in valid `<speak>...</speak>` tags with the correct language attribute (derived automatically from config). \"\n","        \"Step 2: Take the **output** from the 'SSML Formatter Tool' and use the 'SSML to Speech Tool' to synthesize this final, validated SSML into a single audio track. \"\n","        f\"Ensure the final audio is saved correctly as a .wav file to the path '{OUTPUT_FILE_PATH}'. \"\n","        \"Report success or failure, including the output path on success or detailed error messages on failure.\"\n","    ),\n","    agent=speech_synthesizer,\n","    expected_output=(\n","        \"Confirmation of successful audio file generation from the formatted SSML input, including the \"\n","        f\"file path where the .wav file was saved (expected: {OUTPUT_FILE_PATH}), or a descriptive error message if any step failed.\"\n","    ),\n","    context=[business_update_task] # Depends on the SSML script\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b12341d-0d08-422f-aabe-9fc3a64582e1"},{"cell_type":"code","source":["# ======================= CREW EXECUTION =======================\n","if __name__ == \"__main__\":\n","    # Make sure global_llm is set\n","    if 'global_llm' not in globals() or global_llm is None:\n","        raise ValueError(\"The 'llm' variable is not defined or not assigned to 'global_llm'. Please ensure your LLM is initialized.\")\n","\n","    print(f\"\\n--- Starting Crew Workflow ---\")\n","    print(f\"Output Audio Path: {OUTPUT_FILE_PATH}\")\n","    print(f\"Host 1 Voice (Azure): {AZURE_SPEECH_VOICE_1}\")\n","    print(f\"Host 2 Voice (Azure): {AZURE_SPEECH_VOICE_2}\")\n","    print(f\"--> Derived SSML Language: {AZURE_SPEECH_LANG}\")\n","    print(f\"----------------------------\\n\")\n","\n","    # --- Updated Crew Definition ---\n","    crew = Crew(\n","        agents=[\n","            daxAgent ,\n","            report_analyzer, \n","            podcast_writer,\n","            speech_synthesizer\n","        ],\n","        tasks=[\n","            dax_task ,\n","            analysis_task, \n","            business_update_task,\n","            speech_task\n","        ],\n","        verbose=2 # Use verbose=2 for detailed logs\n","    )\n","    # --- End Updated Crew Definition ---\n","\n","    # Execute the crew workflow\n","    print(\"\\n--- Kicking off Crew ---\")\n","    result = crew.kickoff()\n","    print(\"\\n--- Crew Workflow Finished ---\")\n","    print(\"Final result from Crew:\", result)\n","\n","    # --- Post-Run Verification (Keep as before) ---\n","    print(f\"\\n--- Post-Run Verification ---\")\n","    if os.path.exists(OUTPUT_FILE_PATH):\n","        print(f\"Success: Output audio file found at: {OUTPUT_FILE_PATH}\")\n","        print(f\"File size: {os.path.getsize(OUTPUT_FILE_PATH)} bytes\")\n","    else:\n","        print(f\"Error: Output audio file NOT found at the expected path: {OUTPUT_FILE_PATH}\")\n","        print(\"Please check the logs above for errors during the analysis, formatting or synthesis tasks.\")\n","    print(f\"---------------------------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c9412df-a7d0-4a76-a04e-c1d453899779"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"5f9213df-9ace-4095-b44a-341ae233c2e5","known_lakehouses":[{"id":"5f9213df-9ace-4095-b44a-341ae233c2e5"}],"default_lakehouse_name":"wwilakehouse","default_lakehouse_workspace_id":"f378d61a-e81c-4f4f-8040-e383da4436c8"}}},"nbformat":4,"nbformat_minor":5}