{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fc952-12e4-446b-be37-3758862e4289",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# The command is not a standard IPython magic command. It is designed for use within Fabric notebooks only.\n",
    "%pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community langchain-openai azure-identity python-dotenv pydub azure-cognitiveservices-speech --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8028a7-3582-4f4b-89d4-9736b0caec78",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import tempfile\n",
    "import time\n",
    "import re # Import regex for SSML cleanup\n",
    "from pathlib import Path\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import tool\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig, ResultReason, CancellationReason, SpeechSynthesisOutputFormat\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae3c47-91f5-4a01-8ef0-c5a8f4508630",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== CONFIGURABLE VARIABLES =====================\n",
    "# --- Input Configuration ---\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"/lakehouse/default/Files/.env\") #Example: \"/lakehouse/default/Files/.env\"\n",
    "# ==================================================================\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf2ff3-56e1-4569-9c7f-6d45bd8fd545",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Set the environment variable 'REQUESTS_CA_BUNDLE' to the path of the certificate bundle provided by certifi\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "# Set the environment variable 'SSL_CERT_FILE' to the path of the certificate bundle provided by certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    " \n",
    "# Print environment variables (remove in production)\n",
    "print(\"Checking environment variables:\")\n",
    "print(f\"Endpoint exists: {'MY_AZURE_OPENAI_ENDPOINT' in os.environ}\")\n",
    "print(f\"API Key exists: {'AZURE_OPENAI_KEY' in os.environ}\")\n",
    "print(f\"Deployment exists: {'AZURE_OPENAI_MODEL_DEPLOYMENT' in os.environ}\")\n",
    " \n",
    "# Get configuration\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_endpoint = os.getenv(\"MY_AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    " \n",
    "# Print configurations (remove sensitive data in production)\n",
    "print(\"\\nConfiguration values:\")\n",
    "print(f\"Endpoint: {azure_endpoint}\")\n",
    "print(f\"Deployment: {deployment_name}\")\n",
    "print(f\"API Key length: {len(api_key) if api_key else 0}\")\n",
    "print(f\"Api version: {api_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc1ee7-56f9-4f2c-9bbf-04d78bcb849a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI model\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    azure_deployment=deployment_name,\n",
    "    api_version=api_version,\n",
    ")\n",
    " \n",
    "# Test the connection\n",
    "try:\n",
    "    print(\"\\nTesting Azure OpenAI connection...\")\n",
    "    response = llm.invoke(\"Hello! This is a test message.\")\n",
    "    print(\"Connection successful!\")\n",
    "    print(f\"Response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(f\"Error type: {type(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb0d95-4cf1-4746-981a-4df9509cad9c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'llm' is already initialized in your environment\n",
    "global_llm = llm # Make sure llm is assigned here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763e6cc-ec5a-4779-b026-03e99c87bc0c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Azure Speech Service Configuration\n",
    "# IMPORTANT: Ensure the environment variables AZURE_SPEECH_KEY and AZURE_SPEECH_REGION are set in your environment.\n",
    "AZURE_SPEECH_ENDPOINT= os.getenv(\"AZURE_SPEECH_ENDPOINT\")\n",
    "AZURE_SPEECH_KEY =  os.getenv(\"AZURE_SPEECH_KEY\") # e.g., set AZURE_SPEECH_KEY=\"your_azure_speech_key\" in the .env file\n",
    "AZURE_SPEECH_REGION =  os.getenv(\"AZURE_SPEECH_REGION\") # e.g., set AZURE_SPEECH_REGION=\"yourazureregion\" in the .env file\n",
    "AZURE_SPEECH_LANG = \"en-US\" # Default fallback\n",
    " \n",
    "# Voice for the speaker\n",
    "# Find available voice names here: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts\n",
    "AZURE_SPEECH_VOICE = \"en-AU-WilliamNeural\" # Example: \"en-US-Emma2:DragonHDLatestNeural\" or \"en-AU-WilliamNeural\"\n",
    "\n",
    "# Derive language automatically from the first voice for SSML tag\n",
    "try:\n",
    "    # Check if AZURE_SPEECH_VOICE is not None or empty before attempting split\n",
    "    if AZURE_SPEECH_VOICE and isinstance(AZURE_SPEECH_VOICE, str) and AZURE_SPEECH_VOICE.startswith('<'):\n",
    "        print(f\"Warning: AZURE_SPEECH_VOICE seems to contain a placeholder '{AZURE_SPEECH_VOICE}'. Using default language '{AZURE_SPEECH_LANG}'. Please replace the placeholder.\")\n",
    "    elif AZURE_SPEECH_VOICE and isinstance(AZURE_SPEECH_VOICE, str):\n",
    "         parts = AZURE_SPEECH_VOICE.split('-')\n",
    "         if len(parts) >= 2:\n",
    "             AZURE_SPEECH_LANG = f\"{parts[0]}-{parts[1]}\"\n",
    "         else:\n",
    "              print(f\"Warning: Could not reliably determine language from voice '{AZURE_SPEECH_VOICE}'. Using default '{AZURE_SPEECH_LANG}'.\")\n",
    "    elif AZURE_SPAZURE_SPEECH_VOICEEECH_VOICE_1 is None:\n",
    "         print(f\"Warning: AZURE_SPEECH_VOICE is not set. Using default language '{AZURE_SPEECH_LANG}'.\")\n",
    " \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error parsing voice name for language '{AZURE_SPEECH_VOICE}': {e}. Using default '{AZURE_SPEECH_LANG}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238a4df-a9bb-42e7-90e5-8c1fa291f68e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the image file to be processed within the Lakehouse environment\n",
    " \n",
    "LOCAL_IMAGE_PATH = \"/lakehouse/default/Files/store_sales.png\" #Example: \"/lakehouse/default/Files/podcastreport/store_incidents.png\"\n",
    "# LOCAL_IMAGE_PATH = \"abfss://CMPT_WS@onelake.dfs.fabric.microsoft.com/PodCastLH.Lakehouse/Files/images/sales_summary.png\"\n",
    "# --- File and Credential Checks ---\n",
    "print(f\"--- Configuration Summary ---\")\n",
    "print(f\"Input Image Path: {LOCAL_IMAGE_PATH}\")\n",
    "if not LOCAL_IMAGE_PATH or LOCAL_IMAGE_PATH.startswith(\"<\"):\n",
    "    print(\"Warning: Input image path appears to be a placeholder or empty. Please update the LOCAL_IMAGE_PATH variable.\")\n",
    "    # Optionally raise an error if you want to force the user to change it:\n",
    "    # raise ValueError(\"Please replace the placeholder value for LOCAL_IMAGE_PATH before running.\")\n",
    "elif not os.path.exists(LOCAL_IMAGE_PATH):\n",
    "    print(f\"Error: Input image file NOT found at: {LOCAL_IMAGE_PATH}\")\n",
    "    raise FileNotFoundError(f\"Could not find image at the specified LOCAL_IMAGE_PATH: {LOCAL_IMAGE_PATH}\")\n",
    "else:\n",
    "    print(\"Input image file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ebb39-833b-4e74-aad2-4e14772c3dd2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "# Directory path where the final audio file will be saved. Ensure this path exists or is writable. Must end with a forward slash '/'.\n",
    "DEFAULT_OUTPUT_FILEPATH = \"/lakehouse/default/Files/podcastreport/\" # Example: \"/lakehouse/default/Files/podcastreport/\"\n",
    "OUTPUT_FILENAME = \"store_sales\" # todo - change to use the name of input image file\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}_{timestamp}\"\n",
    "# Construct the full output file path\n",
    "DEFAULT_OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}.wav\"\n",
    "OUTPUT_FILE_PATH = DEFAULT_OUTPUT_FILEPATH + DEFAULT_OUTPUT_FILENAME\n",
    "\n",
    "# MIME type mappings (Generally constant)\n",
    "MIME_TYPES = {\n",
    "    \".png\": \"image/png\",\n",
    "    \".jpg\": \"image/jpeg\",\n",
    "    \".jpeg\": \"image/jpeg\",\n",
    "    \".gif\": \"image/gif\",\n",
    "    \".webp\": \"image/webp\"\n",
    "}\n",
    "# =================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3c131-2e7c-44ff-9807-8e78bcdadae0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Azure Speech Region: {AZURE_SPEECH_REGION}\")\n",
    "if not AZURE_SPEECH_KEY:\n",
    "    print(\"Warning: AZURE_SPEECH_KEY environment variable not found or empty.\")\n",
    "    # Optionally raise an error\n",
    "    # raise ValueError(\"Azure Speech Key (AZURE_SPEECH_KEY) environment variable not set.\")\n",
    "if not AZURE_SPEECH_REGION:\n",
    "    print(\"Warning: AZURE_SPEECH_REGION environment variable not found or empty.\")\n",
    "    # Optionally raise an error\n",
    "    # raise ValueError(\"Azure Speech Region (AZURE_SPEECH_REGION) environment variable not set.\")\n",
    " \n",
    "print(f\"Speaker Voice: {AZURE_SPEECH_VOICE}\")\n",
    "print(f\"Derived SSML Language: {AZURE_SPEECH_LANG}\")\n",
    "print(f\"Output Base Filename: {OUTPUT_FILENAME}\")\n",
    "print(f\"Output Directory: {DEFAULT_OUTPUT_FILEPATH}\")\n",
    "print(f\"Full Output Path: {OUTPUT_FILE_PATH}\")\n",
    "print(f\"----------------------------\")\n",
    " \n",
    "# Pre-flight check for output directory\n",
    "output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    print(f\"Warning: Output directory '{output_dir}' does not exist. Attempting to create it.\")\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True) # Use exist_ok=True\n",
    "        print(f\"Successfully created output directory: {output_dir}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Failed to create output directory '{output_dir}': {e}\")\n",
    "        raise # Re-raise the error to stop execution if directory creation fails\n",
    "# --- End Checks ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52480039-fe2e-48b5-83ae-689423eb5316",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@tool(\"Image Description Tool\")\n",
    "def describe_image(local_image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes and describes images in detail.\n",
    "    Provide the full path to the image file you want to describe.\n",
    "    \"\"\"\n",
    "    # --- (describe_image tool) ---\n",
    "    try:\n",
    "        # Access the global LLM\n",
    "        llm = global_llm\n",
    "        if llm is None:\n",
    "             return \"Error: LLM not provided to the tool. Make sure to set global_llm before using this tool.\"\n",
    "        # Print debug info\n",
    "        print(f\"Tool received path: {local_image_path}\")\n",
    "        # Convert to Path object\n",
    "        image_path = Path(local_image_path)\n",
    "        # Double-check file exists\n",
    "        if not image_path.exists():\n",
    "            print(f\"Error: Image file not found at {image_path}\")\n",
    "            return f\"Error: Image file not found at {image_path}\"\n",
    "        else:\n",
    "            print(f\"Tool confirmed file exists at {image_path}\")\n",
    "        # Read and encode image to base64\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            img_data = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "            file_size = os.path.getsize(image_path)\n",
    "            print(f\"Successfully read image file. Size: {file_size} bytes\")\n",
    "        # Get the MIME type\n",
    "        extension = image_path.suffix.lower()\n",
    "        mime_type = MIME_TYPES.get(extension, \"application/octet-stream\")\n",
    "        print(f\"Using MIME type: {mime_type}\")\n",
    "        # Format image content for Azure OpenAI\n",
    "        image_content = f\"data:{mime_type};base64,{img_data}\"\n",
    "        # Use the image_content with Azure OpenAI\n",
    "        print(\"Sending image to Azure OpenAI for description...\")\n",
    "        try:\n",
    "            # Create a message with the image\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Please describe this image in detail, focusing on any text, charts, graphs, or data presented:\" # Slightly tuned prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_content\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            # Get the response from Azure OpenAI\n",
    "            response = llm.invoke([message])\n",
    "            print(\"Received response from Azure OpenAI\")\n",
    "            # Extract the content from the response\n",
    "            if isinstance(response, AIMessage):\n",
    "                return response.content\n",
    "            else:\n",
    "                return str(response)\n",
    "        except Exception as llm_error:\n",
    "            print(f\"LLM error: {str(llm_error)}\")\n",
    "            return f\"Error getting image description from LLM: {str(llm_error)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tool: {str(e)}\")\n",
    "        return f\"Error processing image: {str(e)}\"\n",
    "    # --- (End of describe_image code) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd11c43-b4b1-42c5-9c01-b0007f6a0c3d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@tool(\"SSML Formatter Tool\")\n",
    "def format_ssml_tool(dialogue_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes podcast dialogue text (expected to contain <voice> tags already)\n",
    "    and wraps it in the necessary <speak> root element for Azure SSML,\n",
    "    dynamically setting the xml:lang attribute based on the configured primary voice.\n",
    "    It cleans up potential extra whitespace or markdown code fences.\n",
    "    Provide the raw dialogue content generated by the scriptwriter.\n",
    "    \"\"\"\n",
    "    # --- (format_ssml_tool) ---\n",
    "    print(\"SSML Formatter Tool: Received dialogue content.\")\n",
    "    ssml_body = str(dialogue_content).strip()\n",
    "    # Remove potential markdown code fences added by LLM\n",
    "    ssml_body = re.sub(r'^```xml\\s*', '', ssml_body, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    ssml_body = re.sub(r'\\s*```$', '', ssml_body, flags=re.MULTILINE)\n",
    "    ssml_body = ssml_body.strip()\n",
    "    lang_attribute = f'xml:lang=\"{AZURE_SPEECH_LANG}\"'\n",
    "    is_already_wrapped = ssml_body.startswith('<speak') and ssml_body.endswith('</speak>')\n",
    "    has_correct_lang = lang_attribute in ssml_body[:150]\n",
    "    if is_already_wrapped and has_correct_lang:\n",
    "        print(f\"SSML Formatter Tool: Input already correctly wrapped with {lang_attribute}.\")\n",
    "        return ssml_body\n",
    "    elif is_already_wrapped and not has_correct_lang:\n",
    "        print(f\"SSML Formatter Tool: Input wrapped, but ensuring correct {lang_attribute}.\")\n",
    "        pattern = re.compile(r'(<speak[^>]*)(\\s*xml:lang=\"[^\"]*\")?([^>]*>)', re.IGNORECASE)\n",
    "        if pattern.search(ssml_body):\n",
    "             ssml_body = pattern.sub(rf'\\1 {lang_attribute}\\3', ssml_body, count=1)\n",
    "        else:\n",
    "             ssml_body = ssml_body.replace('<speak', f'<speak {lang_attribute}', 1)\n",
    "        return ssml_body\n",
    "    else:\n",
    "        print(f\"SSML Formatter Tool: Wrapping dialogue content with {lang_attribute}.\")\n",
    "        ssml_full = f'<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" {lang_attribute}>\\n{ssml_body}\\n</speak>'\n",
    "        return ssml_full\n",
    "    # --- (End of format_ssml_tool code) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bce97-f8f3-46fa-b8a6-486133887b18",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "@tool(\"SSML to Speech Tool\")\n",
    "def ssml_to_speech(ssml_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a **complete and valid** SSML string (including the root <speak> tag\n",
    "    with correct xml:lang, and <voice> tags defining multiple voices) into speech\n",
    "    and saves it as a single WAV file. Provide the full, validated SSML string.\n",
    "    Use the 'SSML Formatter Tool' first to ensure validity.\n",
    "    \"\"\"\n",
    "    # --- (Keep the ssml_to_speech tool code as corrected previously) ---\n",
    "    # ... (code from previous version for ssml_to_speech) ...\n",
    "    try:\n",
    "        # Credentials check\n",
    "        speech_config = SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)\n",
    "        speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)\n",
    "        ssml_content = str(ssml_input)\n",
    "        if not ssml_content.strip().startswith('<speak'):\n",
    "            print(f\"Warning in ssml_to_speech: Input SSML might be invalid (missing <speak> tag): {ssml_content[:100]}...\")\n",
    "        print(f\"SSML to Speech Tool: Processing SSML (language should be embedded within): '{ssml_content[:150]}...'\")\n",
    "        print(f\"Output will be saved to: {OUTPUT_FILE_PATH}\")\n",
    "        if not ssml_content:\n",
    "            return \"Error: No SSML content provided for speech synthesis.\"\n",
    "        output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir); print(f\"Created output directory: {output_dir}\")\n",
    "        audio_config = AudioConfig(filename=OUTPUT_FILE_PATH)\n",
    "        synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "        print(\"Synthesizing SSML...\")\n",
    "        result = synthesizer.speak_ssml_async(ssml_content).get()\n",
    "        del synthesizer; del audio_config; time.sleep(0.1)\n",
    "        if result.reason == ResultReason.SynthesizingAudioCompleted:\n",
    "            wait_time = 0\n",
    "            while not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0) and wait_time < 5:\n",
    "                time.sleep(0.5); wait_time += 0.5\n",
    "            if not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0):\n",
    "                 return f\"Error: Output file {OUTPUT_FILE_PATH} is missing or empty after synthesis completed.\"\n",
    "            else:\n",
    "                 print(f\"SSML synthesis successful. File saved to {OUTPUT_FILE_PATH} ({os.path.getsize(OUTPUT_FILE_PATH)} bytes).\")\n",
    "                 return f\"Speech synthesis successful. File saved at: {OUTPUT_FILE_PATH}\"\n",
    "        elif result.reason == ResultReason.Canceled:\n",
    "            cancellation_details = result.cancellation_details\n",
    "            error_message = f\"SSML synthesis canceled. Reason: {cancellation_details.reason}\"\n",
    "            if cancellation_details.reason == CancellationReason.Error:\n",
    "                print(f\"Cancellation Error Code: {cancellation_details.error_code}\")\n",
    "                print(f\"Cancellation Error Details: {cancellation_details.error_details}\")\n",
    "                error_message += f\" Error details: {cancellation_details.error_details}\"\n",
    "            print(error_message)\n",
    "            if os.path.exists(OUTPUT_FILE_PATH):\n",
    "                try: os.remove(OUTPUT_FILE_PATH)\n",
    "                except OSError as remove_err: print(f\"Warning: Could not remove partially created file {OUTPUT_FILE_PATH}: {remove_err}\")\n",
    "            return error_message\n",
    "        else:\n",
    "             error_message = f\"Unexpected SSML synthesis result: {result.reason}\"\n",
    "             print(error_message)\n",
    "             if os.path.exists(OUTPUT_FILE_PATH):\n",
    "                 try: os.remove(OUTPUT_FILE_PATH)\n",
    "                 except OSError as remove_err: print(f\"Warning: Could not remove potentially invalid file {OUTPUT_FILE_PATH}: {remove_err}\")\n",
    "             return error_message\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in ssml_to_speech function: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        if os.path.exists(OUTPUT_FILE_PATH):\n",
    "            try: os.remove(OUTPUT_FILE_PATH)\n",
    "            except OSError as remove_err: print(f\"Warning: Could not remove file {OUTPUT_FILE_PATH} during exception handling: {remove_err}\")\n",
    "        return f\"Error during SSML to speech processing: {str(e)}\"\n",
    "    # --- (End of ssml_to_speech code) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935dafb7-8e06-4aa7-9abc-b922f21d1e1c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ======================= AGENTS =======================\n",
    "image_analyst = Agent(\n",
    "    role=\"Image Analyst\",\n",
    "    goal=\"Provide comprehensive and objective textual descriptions of images, capturing all relevant visual details, especially focusing on text, charts, graphs, or data presented.\",\n",
    "    backstory=\"You're an expert at analyzing and describing visual content with meticulous attention to detail, skilled at extracting factual information from images.\",\n",
    "    verbose=True, allow_delegation=False, llm=global_llm, tools=[describe_image]\n",
    ")\n",
    "# --- NEW Report Analyzer Agent ---\n",
    "report_analyzer = Agent(\n",
    "    role=\"Business Report Analyzer\",\n",
    "    goal=(\n",
    "        \"Analyze the provided detailed image description (containing text, charts, data etc.) \"\n",
    "        \"to extract key business insights, trends, figures, and potential implications. \"\n",
    "        \"Synthesize these findings into a structured written analysis presented as an essay, \"\n",
    "        \"starting with a concise Executive Summary.\"\n",
    "        ),\n",
    "    backstory=(\n",
    "        \"You are a meticulous business analyst skilled at interpreting data presented visually (via its textual description). \"\n",
    "        \"You excel at identifying significant patterns, summarizing complex information clearly, \"\n",
    "        \"and presenting actionable findings in a well-structured report format suitable for executive review and communication planning.\"\n",
    "        ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=global_llm\n",
    "    # This agent primarily works on text context, no specific tool needed beyond LLM capabilities.\n",
    ")\n",
    "# --- Podcast Writer Agent (Now takes analysis as input) ---\n",
    "podcast_writer = Agent(\n",
    "    role=\"Podcast Dialogue Creator\",\n",
    "    goal=(\n",
    "        \"My main goal is to take that detailed business analysis report and turn it into a friendly, engaging chat between two podcast hosts. \"\n",
    "        \"I'll write the script so it sounds natural, making sure to tag the host's lines correctly (using `<voice name='...'>` tags with the specific voices: \"\n",
    "        f\"'{AZURE_SPEECH_VOICE}' for Host 1 so the final audio sounds great. \"\n",
    "        \"I'll also do my best to wrap the whole thing in the main `<speak>` tags needed for the audio generation step.\"\n",
    "    ),\n",
    "    backstory=(\n",
    "        \"Think of me as your creative partner who's great at taking serious reports and making them easy to understand and interesting to listen to. \"\n",
    "        \"I specialize in writing natural-sounding analytical monologues, making sure all the important points from the analysis are covered clearly. \"\n",
    "        \"I know how to set up the script with the right formatting (like those `<voice>` tags) so it's ready for the next step of actually creating the audio.\"\n",
    "    ),\n",
    "    verbose=True, allow_delegation=False, llm=global_llm\n",
    ")\n",
    "# --- Speech Synthesizer Agent (Keep as before) ---\n",
    "speech_synthesizer = Agent(\n",
    "    role=\"SSML Formatting and Speech Synthesis Orchestrator\",\n",
    "    goal=(\n",
    "        \"Take raw podcast monologue, format it into valid SSML using the SSML Formatter Tool (which sets the correct language based on configuration), \"\n",
    "        \"and then convert the finalized SSML into a single high-quality spoken audio file using the SSML to Speech Tool.\"\n",
    "        ),\n",
    "    backstory=(\n",
    "        \"You are an expert workflow manager for audio production. You first ensure the script is perfectly formatted as SSML with the correct language, \"\n",
    "        \"then you use Azure's Text-to-Speech service to generate seamless, multi-voice audio output from the validated SSML.\"\n",
    "        ),\n",
    "    verbose=True, llm=global_llm, tools=[format_ssml_tool, ssml_to_speech]\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac329ef2-d870-4b1b-a6ee-6042e3e383fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ======================= TASKS =======================\n",
    " \n",
    "image_task = Task(\n",
    "    description=(\n",
    "        f\"Analyze the image located at {LOCAL_IMAGE_PATH}. Provide a comprehensive and objective textual description \"\n",
    "        \"detailing all visible elements, scenes, objects, text, charts, graphs, and any discernible context or data presented visually. \"\n",
    "        \"This description will be used by the Business Report Analyzer.\"\n",
    "    ),\n",
    "    agent=image_analyst,\n",
    "    expected_output=\"A detailed, objective textual description of the image's contents, focusing on data and textual elements.\"\n",
    ")\n",
    " \n",
    "# --- Analysis Task ---\n",
    "analysis_task = Task(\n",
    "    description=(\n",
    "        \"Review the detailed image description provided in the context. \"\n",
    "        \"Based *solely* on the information presented in that description (text, data points, chart descriptions etc.), \"\n",
    "        \"perform a thorough business analysis. Identify key insights, significant trends, important figures, and potential implications. \"\n",
    "        \"Structure your findings as a written essay. Start with a clear 'Executive Summary' section summarizing the main points, \"\n",
    "        \"followed by a more detailed 'Analysis' section elaborating on the findings. \"\n",
    "        \"This report will be used to create a podcast script.\"\n",
    "    ),\n",
    "    agent=report_analyzer,\n",
    "    expected_output=(\n",
    "        \"A well-structured written analysis in essay format. \"\n",
    "        \"It MUST begin with an 'Executive Summary' section. \"\n",
    "        \"It MUST be followed by a detailed 'Analysis' section. \"\n",
    "        \"The analysis must be based ONLY on the information from the input image description.\"\n",
    "    ),\n",
    "    context=[image_task] # Depends on the image description\n",
    ")\n",
    " \n",
    "# --- Business Update Task (it depends on analysis_task) ---\n",
    "business_update_task = Task(\n",
    "    description=(\n",
    "        \"Okay, I have the business analysis report here (check the context). My task is to translate the key findings and insights from this report \"\n",
    "        \"into a natural, monologue for a podcast featuring the host. The conversation should flow well and make the analysis easy for listeners to grasp. \"\n",
    "        \"\\n\\n**Formatting Guide for Audio:**\\n\" # Use markdown for emphasis\n",
    "        f\"*   **Host 1:** Use the `...` tag for everything Host 1 says.\\n\"\n",
    "        #f\"*   **Host 2:** Use the `...` tag for everything Host 2 says.\\n\"\n",
    "        \"*   **Structure:** Please alternate between these tags as the hosts speak. Getting these voice tags right is crucial for the audio step!\\n\"\n",
    "        \"*   **Wrapping (Optional but helpful):** If you can, please also wrap the entire dialogue within the main `...` tags.\\n\"\n",
    "        \"*   **Content:** Just include the words the hosts will actually say. No extra labels like 'Host 1:', notes about music, or markdown formatting around the SSML itself.\"\n",
    "    ),\n",
    "    agent=podcast_writer,\n",
    "    expected_output=(\n",
    "         \"A complete podcast script formatted as monologue. It should feature alternating \"\n",
    "         f\"`...` and `...` tags \"\n",
    "         \"containing the spoken lines for the host, accurately reflecting the input analysis in a monologue style. \"\n",
    "         \"Ideally, the whole script will be enclosed in `...` tags.\"\n",
    "    ),\n",
    "    context=[analysis_task] # Depends on the analysis report\n",
    ")\n",
    " \n",
    "# --- Speech Task (it depends on business_update_task) ---\n",
    "speech_task = Task(\n",
    "    description=(\n",
    "        \"Process the podcast monologue script received from the previous task (available in context). \"\n",
    "        \"Step 1: Use the 'SSML Formatter Tool' to ensure the script is wrapped in valid `...` tags with the correct language attribute (derived automatically from config). \"\n",
    "        \"Step 2: Take the **output** from the 'SSML Formatter Tool' and use the 'SSML to Speech Tool' to synthesize this final, validated SSML into a single audio track. \"\n",
    "        f\"Ensure the final audio is saved correctly as a .wav file to the path '{OUTPUT_FILE_PATH}'. \"\n",
    "        \"Report success or failure, including the output path on success or detailed error messages on failure.\"\n",
    "    ),\n",
    "    agent=speech_synthesizer,\n",
    "    expected_output=(\n",
    "        \"Confirmation of successful audio file generation from the formatted SSML input, including the \"\n",
    "        f\"file path where the .wav file was saved (expected: {OUTPUT_FILE_PATH}), or a descriptive error message if any step failed.\"\n",
    "    ),\n",
    "    context=[business_update_task] # Depends on the SSML script\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ec412-76b1-4c32-95f5-edc8c9dce4c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ======================= CREW EXECUTION =======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure global_llm is set\n",
    "    if 'global_llm' not in globals() or global_llm is None:\n",
    "        raise ValueError(\"The 'llm' variable is not defined or not assigned to 'global_llm'. Please ensure your LLM is initialized.\")\n",
    " \n",
    "    print(f\"\\n--- Starting Crew Workflow ---\")\n",
    "    print(f\"Image Path: {LOCAL_IMAGE_PATH}\")\n",
    "    print(f\"Output Audio Path: {OUTPUT_FILE_PATH}\")\n",
    "    print(f\"Speaker Voice (Azure): {AZURE_SPEECH_VOICE}\")\n",
    "    print(f\"--> Derived SSML Language: {AZURE_SPEECH_LANG}\")\n",
    "    print(f\"----------------------------\\n\")\n",
    " \n",
    "    # --- Updated Crew Definition ---\n",
    "    crew = Crew(\n",
    "        agents=[\n",
    "            image_analyst,\n",
    "            report_analyzer, # Added agent\n",
    "            podcast_writer,\n",
    "            speech_synthesizer\n",
    "        ],\n",
    "        tasks=[\n",
    "            image_task,\n",
    "            analysis_task, # Added task\n",
    "            business_update_task,\n",
    "            speech_task\n",
    "        ],\n",
    "        verbose=2 # Use verbose=2 for detailed logs\n",
    "    )\n",
    "    # --- End Updated Crew Definition ---\n",
    " \n",
    "    # Execute the crew workflow\n",
    "    print(\"\\n--- Kicking off Crew ---\")\n",
    "    result = crew.kickoff()\n",
    "    print(\"\\n--- Crew Workflow Finished ---\")\n",
    "    print(\"Final result from Crew:\", result)\n",
    " \n",
    "    # --- Post-Run Verification (Keep as before) ---\n",
    "    print(f\"\\n--- Post-Run Verification ---\")\n",
    "    if os.path.exists(OUTPUT_FILE_PATH):\n",
    "        print(f\"Success: Output audio file found at: {OUTPUT_FILE_PATH}\")\n",
    "        print(f\"File size: {os.path.getsize(OUTPUT_FILE_PATH)} bytes\")\n",
    "    else:\n",
    "        print(f\"Error: Output audio file NOT found at the expected path: {OUTPUT_FILE_PATH}\")\n",
    "        print(\"Please check the logs above for errors during the analysis, formatting or synthesis tasks.\")\n",
    "    print(f\"---------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "5f9213df-9ace-4095-b44a-341ae233c2e5",
    "default_lakehouse_name": "wwilakehouse",
    "default_lakehouse_workspace_id": "f378d61a-e81c-4f4f-8040-e383da4436c8",
    "known_lakehouses": [
     {
      "id": "5f9213df-9ace-4095-b44a-341ae233c2e5"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
